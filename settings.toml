population_n = 24 # one-half this for each of pedestrians and vehicles
pedestrian_n = 0 # those on map at one time
vehicle_n = 1
total_health = 250 # targeting death rate of 1 in 80*365 deaths in 5.4e6 collisions
heal_reward = 35

forward_accel = 1
backward_accel = 1
max_forward_vel = 3
max_backward_vel = 1
brake_power = 2

# for repeatable testing
timestep_limit = 6000000
# when the timstep reaches this, reset all death, etc counters to 0.
reset_counters_at_timestep = 5000000

# for a single agent type setup
use_single_agent_type = true
habit_theory = 1.0
folk_theory = 0.05
internalization_theory = 1.0
choice_restriction_theory = false

# for the intrinsic moral value search processes
off_path_penalty = 1 # per pixel, vehicle on sidewalk, pedestrian on road (not sidewalk)
off_lane_penalty = 0.75
damage_penalty = 1 # damage multiplier for setting scale
# if an agent is "stuck" -- the best option is to not move
# then actions no worse than this can be additionally considered
moral_forgiveness = -8
# a penalty for when an agent sees itself as stuck (repeating pose/velocity) in the search
stuck_penalty = 10 # unused right now!!!

# for the A-star search processes
astar_depth = 2
search_trip_complete_reward = 100

# for the MDP/Q-Learning processses
state_avoid_dist = 10
explore_one_in_max = 1000
explore_consecutive_limit = 1
# We measure exploration by the number of zero entries in a specific part
# of the q avoid state table. So this value will depend highly on state_avoid_dist
# full exploration fills the table the fastest, but obviously is completely dumb!
full_explore_until = 9500
q_learn_rate = 0.3
q_discount = 0.95
trip_reward = 100
damage_coef = 3
# we use a sort of "simulated annealing" to affect the soft-max probabilistic
# best choice. damage_coef effectively will be divided by the anneal factor when
# the number of zero entries in the  q-avoid-table is above the starting number.
# when it reaches the end number then the damage_coef is as specified above.
anneal_factor = 12
anneal_start = 3500 #7500 #800
anneal_end = 700 #5800 #2100 #300

# NOTE!!! We could also try an Upper Confidaence Bound action selection!

debug_choices_after = 730 #4000 #520

# 400 worked well for q learning, but other stuff needs less
fast_steps_per_update = 10 # number of timesteps between chances for visualization output
slow_step_ms = 20 # real time per simulated timestep for visualization

 # num timesteps between q avoid table empties recalculation,
 # which is used to evalute whether we are done learning the basic Q values.
fast_steps_update_q_empties = 20000
