population_n = 24 # one-half this for each of pedestrians and vehicles
pedestrian_n = 0 # those on map at one time
vehicle_n = 1

# for repeatable testing
timestep_limit = 5000000

# for the MDP/Q-Learning processses
state_avoid_dist = 10
explore_one_in_max = 1000
explore_consecutive_limit = 1
# We measure exploration by the number of zero entries in a specific part
# of the q avoid state table. So this value will depend highly on state_avoid_dist
# full exploration fills the table the fastest, but obviously is completely dumb!
full_explore_until = 950
q_learn_rate = 0.2
q_discount = 0.95
trip_reward = 100
damage_coef = 0

forward_accel = 1
backward_accel = 1
max_forward_vel = 3
max_backward_vel = 3
brake_power = 2

heal_reward = 10

fast_steps_per_update = 400 # number of timesteps between chances for visualization output
slow_step_ms = 20 # real time per simulated timestep for visualization

 # num timesteps between q avoid table empties recalculation,
 # which is used to evalute whether we are done learning the basic Q values.
fast_steps_update_q_empties = 20000
