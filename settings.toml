population_n = 24 # one-half this for each of pedestrians and vehicles
pedestrian_n = 6 # those on map at one time
vehicle_n = 2
building_n = 3
total_health = 250 # targeting death rate of 1 in 80*365 deaths in 5.4e6 collisions
heal_reward = 35

forward_accel = 1
backward_accel = 1
max_forward_vel = 3
max_backward_vel = 1
brake_power = 2

# for repeatable testing
timestep_limit = 5025000
# when the timstep reaches this, reset all death, etc counters to 0.
reset_counters_at_timestep = 5000000
finish_qlearning_at_timestep = 5000000

# For sweeping parameters in the test rig setup
perform_test = true
testing_timesteps = 250000
sweep_weight_min = 0.0
sweep_weight_max = 4.0
sweep_divisions = 8
parallel_tests_n = 24

# for a single agent type setup
use_single_agent_type = true
habit_theory =  1.0
folk_theory = 1.0
internalization_theory = 2.0
choice_restriction_theory = true

# for the intrinsic moral value search processes
# total penalities if the entire vehicle/pedestrian is off
# applied by percentage
# (when vehicle on sidewalk, pedestrian on road (not sidewalk))
off_path_penalty = 2
off_lane_penalty = 1
yeild_crosswalk_penalty = 2
# the above are explicitly for vehicles, this scalar can give a different
# weight/multiplier can modify the above for pedestrians, (1 makes no effect)
pedestrian_off_weight = 0.5
damage_penalty = 2 # damage multiplier for setting scale
# if an agent is "stuck" -- the best option is to not move
# then actions no worse than this can be additionally considered
moral_forgiveness = -8
# a penalty for when an agent sees itself as stuck (repeating pose/velocity) in the search
stuck_penalty = 10 # unused right now!!!

# for the A-star search processes
astar_depth = 3
search_trip_complete_reward = 100
folk_damage_penalty = 1
folk_distance_weight = 0.1 # used by the distance to goal heuristic

# for the MDP/Q-Learning processses
state_avoid_dist = 10
explore_one_in_max = 1000
explore_consecutive_limit = 1
# We measure exploration by the number of zero entries in a specific part
# of the q avoid state table. So this value will depend highly on state_avoid_dist
# full exploration fills the table the fastest, but obviously is completely dumb!
full_explore_until = 9500
q_learn_rate = 0.3
q_discount = 0.95
trip_reward = 100
damage_coef = 3
# we use a sort of "simulated annealing" to affect the soft-max probabilistic
# best choice.
# the factor is basically exp(1/temp) with temp=start_temp*(1 - steps/end_steps)
anneal_start_temp = 12
anneal_end_timesteps = 5000000

# NOTE!!! We could also try an Upper Confidaence Bound action selection!

debug_choices_after = 730 #4000 #520

# 400 worked well for q learning, but other stuff needs less
fast_steps_per_update = 200 # number of timesteps between chances for visualization output
slow_step_ms = 20 # real time per simulated timestep for visualization

 # num timesteps between q avoid table empties recalculation,
 # which is used to evalute whether we are done learning the basic Q values.
fast_steps_update_q_empties = 20000
